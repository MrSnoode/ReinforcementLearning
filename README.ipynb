{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import sys\n",
    "sys.path.append(\"SamplesSource\")\n",
    "sys.path.append(\"Source\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from CleanBotEnv import CleanBotEnv\n",
    "from Models.KerasModel import KerasModel\n",
    "from Models.TableModel import TableModel\n",
    "from Methods.TemporalDifference import Sarsa\n",
    "from Policies import EpsilonGreedyPolicy, GreedyPolicy\n",
    "from KerasModelBuilders import conv1_model\n",
    "from Utilities.Eval import validate_policy\n",
    "from Experiments.Experiment import Experiment, DefaultSuite\n",
    "from Experiments.ExperimentRunner import run_experiment_suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dophin RL\n",
    "\n",
    "Dolphin RL is a reinforcement learning library that is built around a pluggable architecture that facilitates experimentation with RL methods and models of the state-value function. It come with an experimentation framework that ensures the experiments are reproducible, and persistent.\n",
    "\n",
    "For an in-depth introduction to RL please refer to [Reinforcement Learning - An Introduction](https://mitpress.mit.edu/books/reinforcement-learning-second-edition) by Richard S. Sutton and Andrew G. Barto.\n",
    "\n",
    "## Getting started:\n",
    "\n",
    "The most basic steps to train a policy are listed below. The [General Training notebook](SamplesSource/GeneralTraining.ipynb) contains a more comprehensive example of how to train and evaluate a policy for solving the CleanBot environment.\n",
    "\n",
    "**Choose an environment**. Dolphin RL users [OpenAI gym](https://gym.openai.com/docs/) to represent environments and comes with the [CleanBot](SamplesSource/CleanBotEnv.py) sample environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CleanBotEnv(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose a model of the state-value function**. The example below uses neural network based representation, which can only approximate the state-value function, but has the advantage that it can generalize experience to states of the environment it has not encountered during training. Alternatively, a [table based model](Source/Models/TableModel.py) could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasModel(env, conv1_model(env), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose a reinforcement learning method and policy**. The example below uses the Sarsa method. It is very flexible in that is works with environments that have very long or infinite episodes, but in its simplest form will likely  converge slower than e.g. a [Monte Carlo method](Source/Methods/MonteCarlo.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_policy = EpsilonGreedyPolicy(model, exploration=0.1)\n",
    "method = Sarsa(env, model, training_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the policy** by running a few episodes and improve the policy in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_count = 100\n",
    "for i in range(episode_count):\n",
    "    method.run_episode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the performance** of the policy by calculating the average reward received over a number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_policy = GreedyPolicy(model)\n",
    "validate_policy(env, validation_policy, episode_count=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation Framework:\n",
    "\n",
    "It is often desirable to compare the performance of different RL methods or the impact of different changing value of meta parameters. The experiments framework helps to ensure that those experiments are reproducible, documented, and results are safely stored on disk.\n",
    "\n",
    "The code below show how to turn the setup above into an experiment to see how different values of alpha affect the speed of learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaExperiment(Experiment):\n",
    "    def __init__(self, alpha):\n",
    "        super().__init__()\n",
    "        self.env = CleanBotEnv(4)\n",
    "        self.model = KerasModel(self.env, conv1_model(self.env), batch_size=64)\n",
    "        self.training_policy = EpsilonGreedyPolicy(self.model, 0.1)\n",
    "        self.testing_policy = GreedyPolicy(self.model)\n",
    "        self.method = Sarsa(self.env, self.model, self.training_policy, alpha=alpha)\n",
    "        self.name = f\"AlphaExperiment-{alpha:.2f}\"\n",
    "    \n",
    "experiments = [{'alpha': a} for a in [0.01, 0.03, 0.05, 0.07, 0.09]]\n",
    "\n",
    "expriment_suite = DefaultSuite(\n",
    "    AlphaExperiment,\n",
    "    experiments,\n",
    "    episode_count=800,\n",
    "    validation_frequency=50,\n",
    "    validation_episode_count=50\n",
    ")\n",
    "run_experiment_suite(expriment_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this experiment suite creates two files per value of *alpha*:\n",
    "\n",
    "- **AlphaExperiment-*alpha*-validation_avg_reward.npy**: The average reward received when applying the policy to the validation instances of the environment\n",
    "- **AlphaExperiment-*alpha*-model.h5**: The Keras model including learned weights and optimizer state\n",
    "\n",
    "The code blow plots the performance of the policy applied to the validation instances of the environment during the progression of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for alpha in [0.01, 0.03, 0.05, 0.07, 0.09]:\n",
    "    name = f\"AlphaExperiment-{alpha:.2f}\"\n",
    "    ax.plot(np.load(f\"{name}-validation_avg_reward.npy\"), label=name)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Documentation/MainPage-Plot.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
